[192.168.253.31] Executing task 'master1'
[192.168.253.31] run: export PATH=$PATH:/opt/bin
[192.168.253.31] run: sudo kubeadm reset -f 
[192.168.253.31] out: [preflight] running pre-flight checks
[192.168.253.31] out: [reset] stopping the kubelet service
[192.168.253.31] out: [reset] unmounting mounted directories in "/var/lib/kubelet"
[192.168.253.31] out: [reset] no etcd manifest found in "/etc/kubernetes/manifests/etcd.yaml". Assuming external etcd
[192.168.253.31] out: [reset] please manually reset etcd to prevent further issues
[192.168.253.31] out: [reset] deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[192.168.253.31] out: [reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[192.168.253.31] out: [reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[192.168.253.31] out: 

[192.168.253.31] run: sudo kubeadm init  --config coreos-k8s/kubeadm-config.yaml
[192.168.253.31] out: [init] using Kubernetes version: v1.12.1
[192.168.253.31] out: [preflight] running pre-flight checks
[192.168.253.31] out: 	[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
[192.168.253.31] out: [preflight/images] Pulling images required for setting up a Kubernetes cluster
[192.168.253.31] out: [preflight/images] This might take a minute or two, depending on the speed of your internet connection
[192.168.253.31] out: [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[192.168.253.31] out: [kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[192.168.253.31] out: [kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[192.168.253.31] out: [preflight] Activating the kubelet service
[192.168.253.31] out: [certificates] Generated front-proxy-ca certificate and key.
[192.168.253.31] out: [certificates] Generated front-proxy-client certificate and key.
[192.168.253.31] out: [certificates] Generated ca certificate and key.
[192.168.253.31] out: [certificates] Generated apiserver-kubelet-client certificate and key.
[192.168.253.31] out: [certificates] Generated apiserver certificate and key.
[192.168.253.31] out: [certificates] apiserver serving cert is signed for DNS names [master1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s.yunwei.edu k8s.yunwei.edu master1.yunwei.edu master2.yunwei.edu master3.yunwei.edu] and IPs [10.96.0.1 192.168.253.31 192.168.253.30 192.168.253.31 192.168.253.32 192.168.253.33 127.0.0.1]
[192.168.253.31] out: [certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[192.168.253.31] out: [certificates] Generated sa key and public key.
[192.168.253.31] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.31] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[192.168.253.31] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[192.168.253.31] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[192.168.253.31] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[192.168.253.31] out: [controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[192.168.253.31] out: [controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[192.168.253.31] out: [controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[192.168.253.31] out: [init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests" 
[192.168.253.31] out: [init] this might take a minute or longer if the control plane images have to be pulled
[192.168.253.31] out: [apiclient] All control plane components are healthy after 25.505356 seconds
[192.168.253.31] out: [uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[192.168.253.31] out: [kubelet] Creating a ConfigMap "kubelet-config-1.12" in namespace kube-system with the configuration for the kubelets in the cluster
[192.168.253.31] out: [markmaster] Marking the node master1 as master by adding the label "node-role.kubernetes.io/master=''"
[192.168.253.31] out: [markmaster] Marking the node master1 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[192.168.253.31] out: [patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master1" as an annotation
[192.168.253.31] out: [bootstraptoken] using token: txzgpa.dgmwlfocaluorzvi
[192.168.253.31] out: [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[192.168.253.31] out: [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[192.168.253.31] out: [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[192.168.253.31] out: [bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[192.168.253.31] out: [addons] Applied essential addon: CoreDNS
[192.168.253.31] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.31] out: [addons] Applied essential addon: kube-proxy
[192.168.253.31] out: 
[192.168.253.31] out: Your Kubernetes master has initialized successfully!
[192.168.253.31] out: 
[192.168.253.31] out: To start using your cluster, you need to run the following as a regular user:
[192.168.253.31] out: 
[192.168.253.31] out:   mkdir -p $HOME/.kube
[192.168.253.31] out:   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.31] out:   sudo chown $(id -u):$(id -g) $HOME/.kube/config
[192.168.253.31] out: 
[192.168.253.31] out: You should now deploy a pod network to the cluster.
[192.168.253.31] out: Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
[192.168.253.31] out:   https://kubernetes.io/docs/concepts/cluster-administration/addons/
[192.168.253.31] out: 
[192.168.253.31] out: You can now join any number of machines by running the following on each node
[192.168.253.31] out: as root:
[192.168.253.31] out: 
[192.168.253.31] out:   kubeadm join k8s.yunwei.edu:8443 --token txzgpa.dgmwlfocaluorzvi --discovery-token-ca-cert-hash sha256:4af32fea36fe7183757defe285d3984a81dae92dfdf42d11adffbc724852464d
[192.168.253.31] out: 
[192.168.253.31] out: 

[192.168.253.31] run: sudo tar zcvf coreos-k8s/master-conf.tgz -T coreos-k8s/m1_ca_files
[192.168.253.31] out: tar: Removing leading `/' from member names
[192.168.253.31] out: /etc/kubernetes/pki/ca.crt
[192.168.253.31] out: /etc/kubernetes/pki/ca.key
[192.168.253.31] out: /etc/kubernetes/pki/sa.key
[192.168.253.31] out: /etc/kubernetes/pki/sa.pub
[192.168.253.31] out: /etc/kubernetes/pki/front-proxy-ca.crt
[192.168.253.31] out: /etc/kubernetes/pki/front-proxy-ca.key
[192.168.253.31] out: /etc/kubernetes/pki/front-proxy-client.crt
[192.168.253.31] out: /etc/kubernetes/pki/front-proxy-client.key
[192.168.253.31] out: /etc/kubernetes/admin.conf
[192.168.253.31] out: 

[192.168.253.31] run: mkdir -p $HOME/.kube
[192.168.253.31] run: sudo cp  /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.31] run: sudo chown $(id -u):$(id -g) $HOME/.kube/config

Done.
Disconnecting from 192.168.253.31... done.
[192.168.253.32] Executing task 'master2'
[192.168.253.32] run: export PATH=$PATH:/opt/bin
[192.168.253.32] run: sudo kubeadm reset -f 
[192.168.253.32] out: [preflight] running pre-flight checks
[192.168.253.32] out: [reset] stopping the kubelet service
[192.168.253.32] out: [reset] unmounting mounted directories in "/var/lib/kubelet"
[192.168.253.32] out: [reset] no etcd manifest found in "/etc/kubernetes/manifests/etcd.yaml". Assuming external etcd
[192.168.253.32] out: [reset] please manually reset etcd to prevent further issues
[192.168.253.32] out: [reset] deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[192.168.253.32] out: [reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[192.168.253.32] out: [reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[192.168.253.32] out: 

[192.168.253.32] put: master-conf.tgz -> /home/core/master-conf.tgz
[192.168.253.32] run: sudo rm -rf /etc/kubernetes
[192.168.253.32] run: sudo tar zxvf master-conf.tgz -C /etc --strip-components 1
[192.168.253.32] out: etc/kubernetes/pki/ca.crt
[192.168.253.32] out: etc/kubernetes/pki/ca.key
[192.168.253.32] out: etc/kubernetes/pki/sa.key
[192.168.253.32] out: etc/kubernetes/pki/sa.pub
[192.168.253.32] out: etc/kubernetes/pki/front-proxy-ca.crt
[192.168.253.32] out: etc/kubernetes/pki/front-proxy-ca.key
[192.168.253.32] out: etc/kubernetes/pki/front-proxy-client.crt
[192.168.253.32] out: etc/kubernetes/pki/front-proxy-client.key
[192.168.253.32] out: etc/kubernetes/admin.conf
[192.168.253.32] out: 

[192.168.253.32] run: sudo kubeadm init  --config coreos-k8s/kubeadm-config.yaml
[192.168.253.32] out: [init] using Kubernetes version: v1.12.1
[192.168.253.32] out: [preflight] running pre-flight checks
[192.168.253.32] out: 	[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
[192.168.253.32] out: [preflight/images] Pulling images required for setting up a Kubernetes cluster
[192.168.253.32] out: [preflight/images] This might take a minute or two, depending on the speed of your internet connection
[192.168.253.32] out: [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[192.168.253.32] out: [kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[192.168.253.32] out: [kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[192.168.253.32] out: [preflight] Activating the kubelet service
[192.168.253.32] out: [certificates] Generated apiserver-kubelet-client certificate and key.
[192.168.253.32] out: [certificates] Generated apiserver certificate and key.
[192.168.253.32] out: [certificates] apiserver serving cert is signed for DNS names [master2 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s.yunwei.edu k8s.yunwei.edu master1.yunwei.edu master2.yunwei.edu master3.yunwei.edu] and IPs [10.96.0.1 192.168.253.32 192.168.253.30 192.168.253.31 192.168.253.32 192.168.253.33 127.0.0.1]
[192.168.253.32] out: [certificates] Using the existing front-proxy-client certificate and key.
[192.168.253.32] out: [certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[192.168.253.32] out: [certificates] Using the existing sa key.
[192.168.253.32] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.32] out: [kubeconfig] Using existing up-to-date KubeConfig file: "/etc/kubernetes/admin.conf"
[192.168.253.32] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[192.168.253.32] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[192.168.253.32] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[192.168.253.32] out: [controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[192.168.253.32] out: [controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[192.168.253.32] out: [controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[192.168.253.32] out: [init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests" 
[192.168.253.32] out: [init] this might take a minute or longer if the control plane images have to be pulled
[192.168.253.32] out: [apiclient] All control plane components are healthy after 0.018575 seconds
[192.168.253.32] out: [uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[192.168.253.32] out: [kubelet] Creating a ConfigMap "kubelet-config-1.12" in namespace kube-system with the configuration for the kubelets in the cluster
[192.168.253.32] out: [markmaster] Marking the node master2 as master by adding the label "node-role.kubernetes.io/master=''"
[192.168.253.32] out: [markmaster] Marking the node master2 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[192.168.253.32] out: [patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master2" as an annotation
[192.168.253.32] out: [bootstraptoken] using token: jrxdy5.2f5gd32abs5mc8aw
[192.168.253.32] out: [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[192.168.253.32] out: [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[192.168.253.32] out: [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[192.168.253.32] out: [bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[192.168.253.32] out: [addons] Applied essential addon: CoreDNS
[192.168.253.32] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.32] out: [addons] Applied essential addon: kube-proxy
[192.168.253.32] out: 
[192.168.253.32] out: Your Kubernetes master has initialized successfully!
[192.168.253.32] out: 
[192.168.253.32] out: To start using your cluster, you need to run the following as a regular user:
[192.168.253.32] out: 
[192.168.253.32] out:   mkdir -p $HOME/.kube
[192.168.253.32] out:   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.32] out:   sudo chown $(id -u):$(id -g) $HOME/.kube/config
[192.168.253.32] out: 
[192.168.253.32] out: You should now deploy a pod network to the cluster.
[192.168.253.32] out: Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
[192.168.253.32] out:   https://kubernetes.io/docs/concepts/cluster-administration/addons/
[192.168.253.32] out: 
[192.168.253.32] out: You can now join any number of machines by running the following on each node
[192.168.253.32] out: as root:
[192.168.253.32] out: 
[192.168.253.32] out:   kubeadm join k8s.yunwei.edu:8443 --token jrxdy5.2f5gd32abs5mc8aw --discovery-token-ca-cert-hash sha256:4af32fea36fe7183757defe285d3984a81dae92dfdf42d11adffbc724852464d
[192.168.253.32] out: 
[192.168.253.32] out: 

[192.168.253.32] run: mkdir -p $HOME/.kube
[192.168.253.32] run: sudo cp  /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.32] run: sudo chown $(id -u):$(id -g) $HOME/.kube/config
[192.168.253.32] run: sudo mkdir /root/.kube;sudo ls /root/.kube
[192.168.253.32] out: mkdir: cannot create directory '/root/.kube': File exists
[192.168.253.32] out: cache  config  http-cache
[192.168.253.32] out: 

[192.168.253.32] run: sudo cp /etc/kubernetes/admin.conf /root/.kube/config
[192.168.253.32] run: echo "export PATH=$PATH:/opt/bin">bashrc
[192.168.253.32] run: sudo cp bashrc /root/.bashrc
[192.168.253.32] run: export PATH=$PATH:/opt/bin
[192.168.253.33] Executing task 'master2'
[192.168.253.33] run: export PATH=$PATH:/opt/bin
[192.168.253.33] run: sudo kubeadm reset -f 
[192.168.253.33] out: [preflight] running pre-flight checks
[192.168.253.33] out: [reset] stopping the kubelet service
[192.168.253.33] out: [reset] unmounting mounted directories in "/var/lib/kubelet"
[192.168.253.33] out: [reset] no etcd manifest found in "/etc/kubernetes/manifests/etcd.yaml". Assuming external etcd
[192.168.253.33] out: [reset] please manually reset etcd to prevent further issues
[192.168.253.33] out: [reset] deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[192.168.253.33] out: [reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[192.168.253.33] out: [reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[192.168.253.33] out: 

[192.168.253.33] put: master-conf.tgz -> /home/core/master-conf.tgz
[192.168.253.33] run: sudo rm -rf /etc/kubernetes
[192.168.253.33] run: sudo tar zxvf master-conf.tgz -C /etc --strip-components 1
[192.168.253.33] out: etc/kubernetes/pki/ca.crt
[192.168.253.33] out: etc/kubernetes/pki/ca.key
[192.168.253.33] out: etc/kubernetes/pki/sa.key
[192.168.253.33] out: etc/kubernetes/pki/sa.pub
[192.168.253.33] out: etc/kubernetes/pki/front-proxy-ca.crt
[192.168.253.33] out: etc/kubernetes/pki/front-proxy-ca.key
[192.168.253.33] out: etc/kubernetes/pki/front-proxy-client.crt
[192.168.253.33] out: etc/kubernetes/pki/front-proxy-client.key
[192.168.253.33] out: etc/kubernetes/admin.conf
[192.168.253.33] out: 

[192.168.253.33] run: sudo kubeadm init  --config coreos-k8s/kubeadm-config.yaml
[192.168.253.33] out: [init] using Kubernetes version: v1.12.1
[192.168.253.33] out: [preflight] running pre-flight checks
[192.168.253.33] out: 	[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
[192.168.253.33] out: [preflight/images] Pulling images required for setting up a Kubernetes cluster
[192.168.253.33] out: [preflight/images] This might take a minute or two, depending on the speed of your internet connection
[192.168.253.33] out: [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[192.168.253.33] out: [kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[192.168.253.33] out: [kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[192.168.253.33] out: [preflight] Activating the kubelet service
[192.168.253.33] out: [certificates] Generated apiserver certificate and key.
[192.168.253.33] out: [certificates] apiserver serving cert is signed for DNS names [master3 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local k8s.yunwei.edu k8s.yunwei.edu master1.yunwei.edu master2.yunwei.edu master3.yunwei.edu] and IPs [10.96.0.1 192.168.253.33 192.168.253.30 192.168.253.31 192.168.253.32 192.168.253.33 127.0.0.1]
[192.168.253.33] out: [certificates] Generated apiserver-kubelet-client certificate and key.
[192.168.253.33] out: [certificates] Using the existing front-proxy-client certificate and key.
[192.168.253.33] out: [certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[192.168.253.33] out: [certificates] Using the existing sa key.
[192.168.253.33] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.33] out: [kubeconfig] Using existing up-to-date KubeConfig file: "/etc/kubernetes/admin.conf"
[192.168.253.33] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[192.168.253.33] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[192.168.253.33] out: [kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[192.168.253.33] out: [controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[192.168.253.33] out: [controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[192.168.253.33] out: [controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[192.168.253.33] out: [init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests" 
[192.168.253.33] out: [init] this might take a minute or longer if the control plane images have to be pulled
[192.168.253.33] out: [apiclient] All control plane components are healthy after 0.014240 seconds
[192.168.253.33] out: [uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[192.168.253.33] out: [kubelet] Creating a ConfigMap "kubelet-config-1.12" in namespace kube-system with the configuration for the kubelets in the cluster
[192.168.253.33] out: [markmaster] Marking the node master3 as master by adding the label "node-role.kubernetes.io/master=''"
[192.168.253.33] out: [markmaster] Marking the node master3 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[192.168.253.33] out: [patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "master3" as an annotation
[192.168.253.33] out: [bootstraptoken] using token: 64grod.ujag5z2gvr1az96e
[192.168.253.33] out: [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[192.168.253.33] out: [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[192.168.253.33] out: [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[192.168.253.33] out: [bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[192.168.253.33] out: [addons] Applied essential addon: CoreDNS
[192.168.253.33] out: [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[192.168.253.33] out: [addons] Applied essential addon: kube-proxy
[192.168.253.33] out: 
[192.168.253.33] out: Your Kubernetes master has initialized successfully!
[192.168.253.33] out: 
[192.168.253.33] out: To start using your cluster, you need to run the following as a regular user:
[192.168.253.33] out: 
[192.168.253.33] out:   mkdir -p $HOME/.kube
[192.168.253.33] out:   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.33] out:   sudo chown $(id -u):$(id -g) $HOME/.kube/config
[192.168.253.33] out: 
[192.168.253.33] out: You should now deploy a pod network to the cluster.
[192.168.253.33] out: Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
[192.168.253.33] out:   https://kubernetes.io/docs/concepts/cluster-administration/addons/
[192.168.253.33] out: 
[192.168.253.33] out: You can now join any number of machines by running the following on each node
[192.168.253.33] out: as root:
[192.168.253.33] out: 
[192.168.253.33] out:   kubeadm join k8s.yunwei.edu:8443 --token 64grod.ujag5z2gvr1az96e --discovery-token-ca-cert-hash sha256:4af32fea36fe7183757defe285d3984a81dae92dfdf42d11adffbc724852464d
[192.168.253.33] out: 
[192.168.253.33] out: 

[192.168.253.33] run: mkdir -p $HOME/.kube
[192.168.253.33] run: sudo cp  /etc/kubernetes/admin.conf $HOME/.kube/config
[192.168.253.33] run: sudo chown $(id -u):$(id -g) $HOME/.kube/config
[192.168.253.33] run: sudo mkdir /root/.kube;sudo ls /root/.kube
[192.168.253.33] out: mkdir: cannot create directory '/root/.kube': File exists
[192.168.253.33] out: cache  config  http-cache
[192.168.253.33] out: 

[192.168.253.33] run: sudo cp /etc/kubernetes/admin.conf /root/.kube/config
[192.168.253.33] run: echo "export PATH=$PATH:/opt/bin">bashrc
[192.168.253.33] run: sudo cp bashrc /root/.bashrc
[192.168.253.33] run: export PATH=$PATH:/opt/bin

Done.
Disconnecting from 192.168.253.32... done.
Disconnecting from 192.168.253.33... done.
[192.168.253.31] Executing task 'dashboard'
[192.168.253.31] run: sed -i "/^\  ports:/i \  type: NodePort"  coreos-k8s/kubernetes-dashboard.yaml
[192.168.253.31] run: kubectl apply -f  coreos-k8s/kubernetes-dashboard.yaml
[192.168.253.31] out: secret/kubernetes-dashboard-certs created
[192.168.253.31] out: serviceaccount/kubernetes-dashboard created
[192.168.253.31] out: role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
[192.168.253.31] out: rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
[192.168.253.31] out: deployment.apps/kubernetes-dashboard created
[192.168.253.31] out: service/kubernetes-dashboard created
[192.168.253.31] out: 

[192.168.253.31] run: kubectl apply -f  coreos-k8s/dashboard-adminuser.yaml
[192.168.253.31] out: serviceaccount/admin-user created
[192.168.253.31] out: 

[192.168.253.31] run: kubectl apply -f  coreos-k8s/dashboard-rolebonding.yaml
[192.168.253.31] out: clusterrolebinding.rbac.authorization.k8s.io/admin-user created
[192.168.253.31] out: 

[192.168.253.31] run: kubectl taint nodes --all node-role.kubernetes.io/master- ;ls
[192.168.253.31] out: taint "node-role.kubernetes.io/master:" not found
[192.168.253.31] out: taint "node-role.kubernetes.io/master:" not found
[192.168.253.31] out: taint "node-role.kubernetes.io/master:" not found
[192.168.253.31] out: config.tgz	etcd1541397515	etcd1541419500	hosts.bak	    user_data
[192.168.253.31] out: coreos-k8s	etcd1541403337	etcdbak		hosts.tmp
[192.168.253.31] out: etcd1541395964	etcd1541405588	ha.tgz		recover-master1.sh
[192.168.253.31] out: 


Done.
Disconnecting from 192.168.253.31... done.
[192.168.253.31] Executing task 'finish'
[192.168.253.31] run: kubectl get svc --all-namespaces -o wide
[192.168.253.31] out: NAMESPACE     NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE   SELECTOR
[192.168.253.31] out: default       kubernetes             ClusterIP   10.96.0.1        <none>        443/TCP         85m   <none>
[192.168.253.31] out: kube-system   calico-etcd            ClusterIP   10.96.232.136    <none>        6666/TCP        81m   k8s-app=calico-etcd
[192.168.253.31] out: kube-system   kube-dns               ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP   85m   k8s-app=kube-dns
[192.168.253.31] out: kube-system   kubernetes-dashboard   NodePort    10.109.115.207   <none>        443:32041/TCP   78m   k8s-app=kubernetes-dashboard
[192.168.253.31] out: 

[192.168.253.31] run: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
[192.168.253.31] out: Name:         admin-user-token-6jqvm
[192.168.253.31] out: Namespace:    kube-system
[192.168.253.31] out: Labels:       <none>
[192.168.253.31] out: Annotations:  kubernetes.io/service-account.name: admin-user
[192.168.253.31] out:               kubernetes.io/service-account.uid: 77b6a9c0-e1c7-11e8-b6a6-525400d8e2cd
[192.168.253.31] out: 
[192.168.253.31] out: Type:  kubernetes.io/service-account-token
[192.168.253.31] out: 
[192.168.253.31] out: Data
[192.168.253.31] out: ====
[192.168.253.31] out: ca.crt:     1025 bytes
[192.168.253.31] out: namespace:  11 bytes
[192.168.253.31] out: token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZqcXZtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3N2I2YTljMC1lMWM3LTExZTgtYjZhNi01MjU0MDBkOGUyY2QiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.mMyhx9Mu1wMu1wpHJjR3UY460oDUWNJLwPmZbXunbP5LLjM7l20dpEAMU6VeHFCcWpJn5PRFBc7gndy5SR6RZln1IRJuXTPnQqLDKvceyB86zs9w6Ex7SqjyF8YOJNM8wPRljCe7-IpKGcMnNmVI3zcfV1AYjd0zPIHrWx1vz0dOmpzAexr6kXPFjeieFB8C2BmTVmG5LyafEqvWN9MIZGa_wK3FpR0wrvnijnYsh-FjHqAyzeRFOBprVq2v_1u4JHQUvJLHLC4JbHs09E7QJpfJtOqCuWIduY436G1lpkfKHm2kIaKRyESmOhjjOc3s13QUbEjNQx8bgNffHrm19A
[192.168.253.31] out: 


Done.
Disconnecting from 192.168.253.31... done.
[192.168.253.31] Executing task 'reboot'
[192.168.253.32] Executing task 'reboot'
[192.168.253.33] Executing task 'reboot'
[192.168.253.34] Executing task 'reboot'
[192.168.253.34] run: sudo reboot
[192.168.253.31] run: sudo reboot
[192.168.253.33] run: sudo reboot
[192.168.253.32] run: sudo reboot
[192.168.253.31] Executing task 'reboot'
[192.168.253.32] Executing task 'reboot'
[192.168.253.33] Executing task 'reboot'
[192.168.253.34] Executing task 'reboot'
[192.168.253.34] run: sudo reboot
[192.168.253.33] run: sudo reboot
[192.168.253.31] run: sudo reboot
[192.168.253.32] run: sudo reboot
[192.168.253.31] Executing task 'finish'
[192.168.253.31] run: kubectl get svc --all-namespaces -o wide
[192.168.253.31] out: NAMESPACE     NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE   SELECTOR
[192.168.253.31] out: default       kubernetes             ClusterIP   10.96.0.1        <none>        443/TCP         18h   <none>
[192.168.253.31] out: kube-system   calico-etcd            ClusterIP   10.96.232.136    <none>        6666/TCP        73m   k8s-app=calico-etcd
[192.168.253.31] out: kube-system   kube-dns               ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP   18h   k8s-app=kube-dns
[192.168.253.31] out: kube-system   kubernetes-dashboard   NodePort    10.109.115.207   <none>        443:32041/TCP   18h   k8s-app=kubernetes-dashboard
[192.168.253.31] out: 

[192.168.253.31] run: kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
[192.168.253.31] out: Name:         admin-user-token-6jqvm
[192.168.253.31] out: Namespace:    kube-system
[192.168.253.31] out: Labels:       <none>
[192.168.253.31] out: Annotations:  kubernetes.io/service-account.name: admin-user
[192.168.253.31] out:               kubernetes.io/service-account.uid: 77b6a9c0-e1c7-11e8-b6a6-525400d8e2cd
[192.168.253.31] out: 
[192.168.253.31] out: Type:  kubernetes.io/service-account-token
[192.168.253.31] out: 
[192.168.253.31] out: Data
[192.168.253.31] out: ====
[192.168.253.31] out: ca.crt:     1025 bytes
[192.168.253.31] out: namespace:  11 bytes
[192.168.253.31] out: token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZqcXZtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3N2I2YTljMC1lMWM3LTExZTgtYjZhNi01MjU0MDBkOGUyY2QiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.mMyhx9Mu1wMu1wpHJjR3UY460oDUWNJLwPmZbXunbP5LLjM7l20dpEAMU6VeHFCcWpJn5PRFBc7gndy5SR6RZln1IRJuXTPnQqLDKvceyB86zs9w6Ex7SqjyF8YOJNM8wPRljCe7-IpKGcMnNmVI3zcfV1AYjd0zPIHrWx1vz0dOmpzAexr6kXPFjeieFB8C2BmTVmG5LyafEqvWN9MIZGa_wK3FpR0wrvnijnYsh-FjHqAyzeRFOBprVq2v_1u4JHQUvJLHLC4JbHs09E7QJpfJtOqCuWIduY436G1lpkfKHm2kIaKRyESmOhjjOc3s13QUbEjNQx8bgNffHrm19A
[192.168.253.31] out: 


Done.
Disconnecting from 192.168.253.31... done.
